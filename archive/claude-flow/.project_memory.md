# Mercury DS Manager Take-Home - Project Memory

Last Updated: 2025-10-30
Status: Configuration complete
Working Directory: /Users/mattstrautmann/Documents/takehomes/mercury/claude-flow/

## Project Quick Facts

Assignment Type: DS Manager take-home (Mercury)
Time Limit: 5 hours maximum
Candidate: Matt Strautmann
Deliverable: Jupyter notebook (exploratory analysis + experiment design)
Config File: CLAUDE.md
Data Location: ../Data_Science_Manager_Take_Home/

## Dataset Quick Reference

Files and Sizes:
- organizations.csv: 500 rows (org demographics)
- adoption_funnel.csv: 2,000 rows (onboarding stages)
- product_usage.csv: 200,000 rows (daily product activity)

Key Relationships:
- organizations (500) → adoption_funnel (2,000) via organization_id
- adoption_funnel → product_usage (200,000) via organization_id
- Long format: each org has 0-4 funnel rows, multiple product usage rows

Critical Data Facts:
- Funnel stages: application_submitted, approved, first_deposit, first_active
- Products: Bank Account (all get), Debit Card, Credit Card (restricted), Invoicing
- Segments: segment_size (micro/small/medium) × segment_growth_potential (low/high)
- Industries: 2-level hierarchy (industry_type → industry)
- Join key: organization_id across all tables

## Analysis Objectives

Part 1: Exploratory Analysis (3-4 hours)
1. Data Understanding: Load and inspect all 3 datasets
2. Industry × Approval: Which industries have highest approval rates?
3. Segment × Adoption: Does growth potential affect product adoption?
4. Product Churn: Define and measure churn patterns
5. Key Insights: Synthesize 3-5 findings for product team
6. Dashboard Concept: Design self-serve exploration tool

Part 2: Experiment Design (1-2 hours)
1. Hypothesis: Industry-specific product recommendations increases adoption
2. Design Decisions: industry_type vs industry segmentation, control/treatment split, randomization, stratification, primary metric, sample size, duration
3. Analysis Plan: Statistical tests, guardrails, subgroups, HTE
4. Decision Framework: Result to action mapping

## Critical Execution Rules

INCREMENTAL WORKFLOW:
- ONE task per notebook cell
- Test with uv run python BEFORE adding to notebook
- Show exploration process (failures, pivots, questions)
- Natural markdown (Let me check..., Hmm, unexpected...)
- Print intermediate results frequently

DO NOT:
- Plan 10 cells ahead
- Write polished presentation on first pass
- Jump to complex models without EDA
- Hide dead-ends or failed attempts
- Use placeholders/TODOs/mock implementations

Cell Development Protocol:
1. Draft code for ONE exploration task
2. Test it: uv run python -c "..."
3. If successful, add to notebook
4. Execute in notebook, inspect output
5. Based on output, decide next step
6. Repeat

## SPARC Methodology Phases

Phase 1: Specification (Data Understanding)
- Load each dataset independently
- Check shapes, dtypes, nulls, distributions
- Understand grain and relationships

Phase 2: Pseudocode (Analysis Planning)
- Outline analytical approach in markdown
- Think through join strategies
- Plan aggregation logic
- ONE step at a time

Phase 3: Architecture (Data Pipelines)
- Design joins (organizations → funnel → usage)
- Create base analytical tables
- Implement transformations incrementally

Phase 4: Refinement (Iterative Analysis)
- Test hypotheses from Part 1 objectives
- Check statistical significance
- Iterate based on findings

Phase 5: Completion (Synthesis)
- Synthesize 3-5 key insights with evidence
- Design dashboard concept
- Create rigorous experiment plan

## Key Research Context

Onboarding Best Practices:
- Brex case study: +29% applications via industry segmentation
- Activation benchmarks: Median 17%, top performers 65%
- TTV: Speed to first value moment
- Personalization: Tailor to industry/role

A/B Testing Framework:
- Stratification: Use when n<400, choose 4-6 strata
- Power: 0.8 standard, α=0.05
- MDE affects sample size
- Three pillars: Statistical significance + practical significance + business impact
- Guardrails: Trust (SRM, data quality) + Organizational (UX, revenue)
- HTE: Different industries may respond differently

Statistical Tests:
- Approval rates: Chi-square test
- Product adoption: Two-sample proportion test
- Time-to-event: Survival analysis (Kaplan-Meier, Cox)
- Subgroup effects: Interaction terms in regression

## Environment

Python:
- ALWAYS use uv
- Test: uv run python -c "import pandas as pd; print(pd.__version__)"
- Install: uv pip install pandas numpy matplotlib seaborn scipy statsmodels jupyter

Directory Structure:
Data_Science_Manager_Take_Home/
├── organizations.csv
├── adoption_funnel.csv
├── product_usage.csv
└── matt_strautmann_mercury_analysis.ipynb  ← SINGLE notebook

claude-flow/
├── CLAUDE.md
├── .project_memory.md
├── coordination/
└── memory/

Single Notebook Structure (~20-30 cells total):
- Part 1: Data Understanding (cells 1-8)
- Part 1: Analysis (cells 9-18)
- Part 1: Insights Summary (cells 19-20)
- Part 2: Experiment Design (cells 21-28)

## Progress Tracking

Phase 1: Data Understanding
- [ ] Load organizations.csv (500 rows, 5 cols)
- [ ] Load adoption_funnel.csv (2,000 rows, 3 cols)
- [ ] Load product_usage.csv (200,000 rows, 4 cols)
- [ ] Explore industry_type distribution
- [ ] Explore segment combinations
- [ ] Verify funnel stages (4 expected)
- [ ] Check date ranges
- [ ] Verify products (4 expected)

Phase 2: Cross-Dataset Analysis
- [ ] Industry × approval rates (statistical significance)
- [ ] Segment × product adoption patterns
- [ ] Funnel stage × product usage timing
- [ ] Product churn definition and measurement
- [ ] Time-based patterns (cohorts)

Phase 3: Insights and Dashboard
- [ ] Insight 1 (with evidence: n, p-value, effect size)
- [ ] Insight 2
- [ ] Insight 3
- [ ] Insight 4 (optional)
- [ ] Insight 5 (optional)
- [ ] Dashboard use cases defined
- [ ] Dashboard metrics/dimensions specified

Phase 4: Experiment Design
- [ ] Hypothesis grounded in Phase 2 findings
- [ ] Segmentation choice justified
- [ ] Treatment logic specified
- [ ] Randomization/stratification designed
- [ ] Primary metric defined
- [ ] Sample size calculated
- [ ] Duration projected
- [ ] Guardrail metrics specified
- [ ] Analysis plan detailed
- [ ] Decision framework mapped

## Style Examples

Good Markdown:
"Let me start by loading the organizations data and seeing what we have..."
"Hmm, I see 12 different industry_types. Let me check how granular the industry field is."
"Interesting - Technology has 78% approval but only 15 orgs. I should check if this is statistically meaningful."

Bad Markdown:
"After comprehensive analysis of the organizational data, we observe that Technology sector demonstrates superior approval metrics."

Good Code Cell:
# Let me check the industry_type distribution
print(orgs['industry_type'].value_counts())

Bad Code Cell:
# Load data, check distributions, calculate approval rates, visualize
orgs = pd.read_csv('organizations.csv')
print(orgs.info())
print(orgs['industry_type'].value_counts())
approval_rates = calculate_approval_by_industry(orgs)
plot_approval_rates(approval_rates)

## Common Pitfalls

Data Analysis:
- Small sample sizes: Check n before claiming significance (need n>30 minimum)
- Multiple comparisons: Apply Bonferroni correction if testing many hypotheses
- Survivorship bias: Orgs in product_usage already passed approval
- Time confounding: Newer orgs have less time to adopt products

Statistical:
- Ignoring power: MDE too small requires huge sample
- Forgetting guardrails: Might improve adoption but hurt approval rate
- Missing HTE: Average effect hides industry-specific differences
- Wrong test: Use chi-square for proportions, t-test for means

Workflow:
- Pre-planning: Don't map out all cells before exploring
- Polished first draft: Show messy exploration, not presentation
- Skipping tests: ALWAYS uv run python before adding to notebook
- Batch operations: One task per cell

## Success Checklist

Notebook Quality:
- [ ] Shows incremental exploration
- [ ] Natural, conversational markdown
- [ ] Prints intermediate results frequently
- [ ] Documents pivots and unexpected findings
- [ ] One task per cell
- [ ] All code tested before adding

Analysis Rigor:
- [ ] Sample sizes reported for all claims
- [ ] Statistical significance tested (p-values)
- [ ] Effect sizes quantified
- [ ] Confidence intervals included
- [ ] Assumptions checked

Insights Quality:
- [ ] 3-5 key findings identified
- [ ] Each insight backed by data (n, p, effect size)
- [ ] Business implications clear
- [ ] Confidence levels stated
- [ ] Actionable recommendations

Experiment Design Quality:
- [ ] Hypothesis grounded in Part 1 findings
- [ ] Segmentation choice justified with data
- [ ] Primary metric defined
- [ ] Sample size calculated
- [ ] Duration realistic
- [ ] Guardrails specified (3-5 metrics)
- [ ] Analysis plan detailed
- [ ] Decision framework maps results to actions

Time Management:
- [ ] Phase 1 completed in ~1.5 hours
- [ ] Phase 2 completed in ~2 hours
- [ ] Phase 3 completed in ~1 hour
- [ ] Phase 4 completed in ~1.5 hours
- [ ] Total ≤ 5 hours

## Reference Files

Internal:
- CLAUDE.md (full configuration)
- ../CLAUDE.md (project root config)
- ../onboarding_experimentation_research.md (background research)

Data:
- ../Data_Science_Manager_Take_Home/organizations.csv
- ../Data_Science_Manager_Take_Home/adoption_funnel.csv
- ../Data_Science_Manager_Take_Home/product_usage.csv

## Mental Model

This is NOT a presentation deck analysis.

This IS an authentic data science exploration:
- Start with simple questions
- Inspect data at each step
- Follow interesting patterns
- Document dead-ends
- Pivot when assumptions fail
- Show the messy process

The interviewer wants to see how you think, not just what you find.
