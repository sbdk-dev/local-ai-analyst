# Mercury Take-Home: Project Memory

**Last Updated**: 2025-10-30
**Status**: Setup complete, ready for analysis

---

## Quick Context

**What**: Data Science Manager take-home for Mercury (B2B banking for startups)
**Time Limit**: 5 hours
**Deliverable**: Jupyter notebook with exploratory analysis + experiment design

---

## Core Files

### Configuration & Research
- **CLAUDE.md** - Primary project configuration with all instructions
- **onboarding_experimentation_research.md** - Background research on:
  - Onboarding flows & product adoption (activation, TTV, churn)
  - A/B testing & experiment design (stratification, HTE, guardrails)
  - Industry benchmarks (17% median activation, 29% Brex lift from personalization)

### Data Files (Data_Science_Manager_Take_Home/)
```
organizations.csv      500 rows × 5 cols      (org demographics)
adoption_funnel.csv    2,000 rows × 3 cols    (funnel stages & dates)
product_usage.csv      200,000 rows × 4 cols  (daily product activity)
```

**Join Key**: `organization_id` (across all tables)

### Reference Material
- **matt_strautmann_GoFundMe_donor_ltv.ipynb** - Style guide for incremental analysis
- **matt_strautmann_mercury_analysis.ipynb** - Realistic 5-hour exploration example (81 cells)

### Natural Data Science Notebook Style

**"Typing as You Think" Pattern**:
- One thought per cell: `orgs.shape` → "500 orgs"
- Learn from output: see 278 → "Same as approved count!"
- Natural observations: "That's high", "Big drop", "Makes sense"

**Natural Language Examples**:
```
✅ "790 null dates. Not everyone completes all stages"
✅ "39.4% activation rate. Big drop from approval to activation"
✅ "Tech way higher on Credit Card (13% vs 3%)"
✅ "79% Bank Account churn. That's high"

❌ "The data indicates that 79% of Bank Account users churn"
❌ "Upon analyzing activation rates, I discovered..."
❌ "Key findings from the funnel analysis:"
```

**Cell Structure**:
- Simple calculations, not complex print statements
- Learn from output in next cell markdown
- Natural progression: basic → specific → insights

---

## Data Schema Quick Reference

### organizations.csv
- `organization_id` - Unique org identifier
- `industry` - Specific industry (granular)
- `industry_type` - Broader industry category
- `segment_size` - micro, small, medium
- `segment_growth_potential` - low, high

### adoption_funnel.csv
- `organization_id` - FK to organizations
- `funnel_stage` - application_submitted, approved, first_deposit, first_active
- `date` - When stage completed (nullable)

**Note**: Long format, each org has 0-4 rows (one per funnel stage reached)

### product_usage.csv
- `organization_id` - FK to organizations
- `day` - Date of event
- `product` - Bank Account, Debit Card, Credit Card, Invoicing
- `is_active` - Boolean (2+ txns/30d for Bank Account, 1+ for others)

**Note**: Daily grain, each org×day×product combination

---

## Two-Part Assignment

### Part 1: Exploratory Analysis
**Goal**: Find 3-5 insights valuable to Experiences product team

Example questions (don't limit to these):
- Which industries have highest approval rates?
- Does growth potential affect product adoption?
- What does product churn look like?

**Deliverables**:
- Exploratory analysis in notebook
- Key findings with supporting data
- Dashboard/app concept design

### Part 2: Experiment Design
**Goal**: Design experiment for industry-specific product recommendations during onboarding

**Hypothesis**: Featuring products based on industry during onboarding will increase early adoption

**Questions to address**:
1. Vary by `industry_type` or `industry`? Why?
2. What's the experiment design?
3. How long to run?
4. How to analyze results?
5. What results → what actions?

**Deliverables**:
- Complete experiment plan grounded in Part 1 findings
- Statistical rigor (sample size, MDE, guardrails, HTE)
- Decision framework (result → action mapping)

---

## Workflow Rules (CRITICAL)

### Two-Step Cell Development
```bash
# Step 1: Test FIRST with uv
uv run python -c "import pandas as pd; df = pd.read_csv('data.csv'); print(df.shape)"

# Step 2: If it works, add to notebook
# [Add tested code to notebook]
```

**Why**: Easier to delete cells later than debug broken notebook

### Incremental Exploration Requirements

✅ **DO**:
- ONE task per cell (load, inspect, plot, calculate)
- Inspect output before deciding next step
- Show failed attempts and pivots
- Natural markdown ("Checking if...", "Unexpected...", just observations)
- Start with .head(), .info(), .describe()
- Print intermediate results frequently
- **ADD VISUALIZATIONS** - plots inform next steps
- **TEST HYPOTHESES** from research (don't cite, use to guide exploration)
- **SHOW DEAD ENDS** - things that didn't work, assumptions invalidated
- **MAKE IT MESSY** - this is 5 hours of real work, not 30min of polished slides

❌ **DON'T**:
- Plan 10 cells ahead based on assignment
- Write polished presentation-ready analysis with formatted tables/bullets
- Jump to complex joins without basic EDA
- Hide exploratory work that didn't yield insights
- Batch multiple operations without checking intermediate results
- **Make it too perfect** - no real analysis has perfect formatting and clear ordered lists
- **Skip interesting questions** - explore patterns beyond the assignment questions
- **Forget to use research** - it guides what to test, not what to cite

---

## Four-Phase Analysis Plan

### Phase 1: Data Understanding (1-1.5h)
- Load each dataset independently
- Check shape, dtypes, nulls, duplicates
- Understand distributions (industry_type, funnel_stage, products)
- Verify data quality

### Phase 2: Cross-Dataset Analysis (1.5-2h)
- Industry × Approval Rates (sample size check)
- Segment × Product Adoption (high growth → more products?)
- Funnel Stage × Product Usage (time to adoption)
- Product Churn Analysis (define churn, rates by segment/industry)

### Phase 3: Key Insights & Dashboard (0.5-1h)
- Synthesize 3-5 findings with supporting data
- Prioritize by business impact
- Include confidence levels (sample sizes, significance)
- Design self-serve dashboard concept

### Phase 4: Experiment Design (1-1.5h)
- Data-driven hypothesis (which industries prefer which products?)
- Variant selection (industry_type vs industry)
- Treatment logic (which product to feature)
- Sample size calculation (power, MDE, baseline rates)
- Analysis plan (primary metric, guardrails, HTE, subgroups)
- Decision framework (result → action mapping)

---

## Key Benchmarks from Research

### Product Adoption
- Median SaaS activation rate: **17%**
- Top performer activation rate: **65%**
- Top SaaS churn rate: **3-5%**
- Banking new account closure: **25-40%** in first year

### Personalization Impact
- Brex industry segmentation lift: **+29%** completed applications
- Customer frustration w/o personalization: **74%**

### Statistical Testing
- Typical power: **0.8** (80%)
- Typical alpha: **0.05** (5%)
- Ideal stratification factors: **4-6 strata**

### Market Context (2024)
- Fintech SaaS market by 2028: **$949B**

---

## Python Environment

**Package Manager**: `uv` (ALWAYS use for all Python operations)

**Common Commands**:
```bash
# Test code
uv run python -c "code here"

# Install packages
uv pip install pandas matplotlib seaborn scipy

# Run Python script
uv run python script.py

# Check pandas version
uv run python -c "import pandas; print(pandas.__version__)"
```

---

## Ready to Start?

1. Test Python environment: `uv run python -c "import pandas; print('Ready!')"`
2. Create notebook: `matt_strautmann_mercury_analysis.ipynb`
3. Begin Phase 1 with organizations.csv
4. Follow incremental workflow (test → add → execute → inspect → decide)
5. Reference research doc for frameworks and benchmarks

**Remember**: This is a DS Manager role. Show realistic exploration process, not polished presentation.
