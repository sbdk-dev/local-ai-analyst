# Semantic Model Design Decisions

**Date**: 2025-11-05
**Phase 2**: Semantic Layer Setup - COMPLETE âœ…

---

## Overview

This document captures key design decisions made while creating semantic models for the AI Analyst system, following Rasmus Engelbrecht's product analytics patterns.

## Data Model: Product Analytics Lifecycle

### Decision 1: Domain Choice

**Decision**: Use product analytics lifecycle data instead of e-commerce
**Rationale**:
- Richer event-based data enables more sophisticated analysis
- Natural time-series patterns (DAU/MAU, retention, cohorts)
- Multiple analysis dimensions (users, features, time)
- Better demonstrations of semantic layer concepts
- Mirrors real SaaS/product company needs

**Alternative Considered**: E-commerce transactional data
**Why Rejected**: Less rich for engagement analysis, fewer statistical testing opportunities

### Decision 2: Three-Model Architecture

**Decision**: Split into 3 semantic models: `users`, `events`, `engagement`
**Rationale**:
- **Users**: Demographics and account details (dimension-rich)
- **Events**: User actions and feature usage (fact table, high volume)
- **Engagement**: Derived metrics requiring complex joins (DAU/MAU, retention)

**Following Rasmus Pattern**: Start with business questions, not schema
- Business teams ask about "user conversion" (users model)
- Product teams ask about "feature adoption" (events model)
- Growth teams ask about "retention trends" (engagement model)

### Decision 3: Measure Definitions

**Key Decisions**:

**Users Model**:
- `conversion_rate` as ratio (paid_users / total_users) - core business metric
- Filtered measures for plan segments (free_users, paid_users, enterprise_users)
- Business context with industry benchmarks (15% median, 35% top quartile)

**Events Model**:
- `events_per_user` as engagement proxy
- `feature_adoption_rate` for product insights
- Time-derived dimensions (event_date, event_hour) for temporal analysis
- Feature-specific measures for detailed adoption tracking

**Engagement Model**:
- Standard SaaS metrics: DAU, MAU, stickiness (DAU/MAU)
- Retention metrics: D1, D7, D30 following industry standards
- Cohort analysis with signup_month dimension
- Business benchmarks for automated interpretation

### Decision 4: Statistical Validation Integration

**Decision**: Embed statistical testing rules directly in semantic models
**Rationale**: Prevents fabrication by auto-running tests when comparing groups

**Implementation**:
```yaml
validation:
  sample_size:
    minimum: 30
    warning_threshold: 100
  significance:
    default_alpha: 0.05
    auto_test_comparisons: true
  effect_size:
    calculate_cohens_h: true
    calculate_relative_diff: true
```

**Example Auto-Test Scenario**:
```
User asks: "How does engagement vary by plan type?"
â†’ AI runs query: events_per_user by plan_type
â†’ System auto-detects group comparison
â†’ System runs ANOVA test
â†’ System calculates effect sizes
â†’ Result: "Enterprise 156 events/user vs Free 14 events/user (p<0.001, large effect, n=34 vs n=596)"
```

### Decision 5: Business Context Layer

**Decision**: Include benchmark data and interpretation rules in semantic models
**Rationale**: Enables AI to provide contextual insights, not just raw numbers

**Implementation Pattern**:
```yaml
context:
  benchmarks:
    - metric: conversion_rate
      industry_median: 0.15
      top_quartile: 0.35
      source: "SaaS conversion benchmarks 2024"

  interpretations:
    conversion_rate:
      excellent: "> 35%"
      good: "20-35%"
      average: "10-20%"
      concerning: "< 10%"
```

**Benefit**: AI can say "Your 40.4% conversion rate is excellent (top quartile: 35%)" instead of just "40.4%"

### Decision 6: Sample Query Inclusion

**Decision**: Include sample queries in each semantic model YAML
**Rationale**:
- Validates model design during development
- Provides reference implementations for complex metrics
- Enables automated testing of semantic layer

**Examples Included**:
- Plan type distribution with percentages
- Feature adoption by industry (cross-model join)
- Daily active user trends
- Cohort retention analysis

---

## Technical Implementation Decisions

### Decision 7: DuckDB as Backend

**Decision**: Use DuckDB for local development database
**Rationale**:
- Zero-config embedded database (no server setup)
- Excellent SQL compatibility
- High performance for analytics workloads
- Native Ibis support
- Easy to distribute (single file database)

**Production Path**: Can migrate to PostgreSQL, BigQuery, Snowflake via Ibis without changing semantic model definitions

### Decision 8: Ibis as Query Layer

**Decision**: Use Ibis for database abstraction layer
**Rationale**:
- Backend-agnostic (can switch from DuckDB to other databases)
- Pythonic query interface
- Strong integration with Boring Semantic Layer
- Excellent for testing semantic model queries

**Validates Rasmus's Architecture**: Semantic Layer â†’ Query Engine â†’ Database

### Decision 9: YAML Schema Design

**Decision**: Follow Boring Semantic Layer YAML conventions
**Rationale**:
- Industry-standard approach
- Native MCP integration available
- Clear separation of dimensions vs measures
- Supports complex joins and derived metrics

**Extensions Added**:
- Statistical validation rules
- Business context and benchmarks
- Sample queries for testing
- Auto-insights configurations

---

## Data Generation Decisions

### Decision 10: Realistic Data Patterns

**Decision**: Generate data with realistic business patterns, not random
**Rationale**: Enables meaningful statistical testing and business insights

**Patterns Implemented**:
- Plan type engagement hierarchy: Enterprise > Pro > Starter > Free
- Industry variation in feature adoption
- Realistic retention curves (high D1, lower D7, much lower D30)
- Business hours activity patterns
- Signup cohort effects

### Decision 11: Sample Sizes for Statistical Validity

**Decision**: Generate 1,000 users, 34K events across 7K sessions
**Rationale**:
- Large enough for statistical significance testing
- Realistic event volume (34 events per user average)
- Sufficient data for cohort analysis across 12 months
- Small enough for fast local development

**Validation**: All industry segments have n>80, enabling reliable statistical comparisons

### Decision 12: Feature Set Design

**Decision**: 10 realistic SaaS features with natural adoption patterns
**Features**: dashboard_view, report_create, report_export, chart_create, data_upload, team_invite, integration_setup, api_call, alert_create, collaboration_edit

**Rationale**:
- Mirrors real product analytics needs
- Natural high-value vs engagement vs activation signals
- Enables realistic adoption analysis by industry/plan

---

## Semantic Layer Philosophy Applied

### Rasmus's Core Principles âœ…

**1. Start with business questions, not schema**
- Users model answers: "What's our conversion rate?"
- Events model answers: "Which features drive engagement?"
- Engagement model answers: "Are we retaining users effectively?"

**2. Metrics should be self-contained and reusable**
- `conversion_rate` = paid_users / total_users (reusable across dimensions)
- `events_per_user` = total_events / unique_users (consistent calculation)
- `d7_retention` = standardized cohort calculation

**3. Dimensions define how metrics can be sliced**
- Plan type: slice any metric by subscription tier
- Industry: understand vertical differences
- Time dimensions: track trends and seasonality

**4. Keep models simple and composable**
- Each model focuses on one business domain
- Clear relationships between models
- Complex metrics (engagement) reference simpler ones (users, events)

### Mercury Project Learnings Applied âœ…

**1. Build â†’ Execute â†’ Annotate**
- All measures tested with real queries before finalizing
- Sample queries validate every semantic concept
- No metrics defined without actual data validation

**2. Statistical Rigor by Default**
- Auto-test rules embedded in model definitions
- Sample size validation prevents unreliable claims
- Effect size calculations for practical significance

**3. Natural Language Interpretation**
- Business context enables conversational responses
- Benchmark comparisons provide meaningful insights
- Interpretation rules guide AI communication

---

## Validation Results

### Phase 2 Testing âœ…

**Database**: 9.5MB DuckDB file with 42K+ records
**Tables**: users (1,000), events (34,348), sessions (7,055)
**Performance**: All queries execute in <100ms locally

**Semantic Model Validation**:
- âœ… All measures calculate correctly
- âœ… Cross-model joins work (users + events)
- âœ… Time-series queries execute (DAU trends)
- âœ… Statistical tests possible (sample sizes adequate)
- âœ… Business context applies (benchmarks â†’ interpretations)

**Key Insights Discovered**:
- 40.4% conversion rate (excellent vs 15% median)
- Clear plan tier engagement hierarchy (Enterprise 156 vs Free 14 events/user)
- Feature adoption varies by industry (collaboration_edit popular across verticals)
- 12.3% stickiness (below 13% median - improvement opportunity)

---

## Next Phase Integration

### Phase 3: MCP Server (Ready)

**Semantic Models â†’ MCP Tools**:
- `query_model('users', ['plan_type'], ['conversion_rate'])` â†’ Returns data + statistical analysis
- `suggest_analysis(context='conversion_rate by plan')` â†’ Returns logical next questions
- `test_significance(result)` â†’ Auto-runs appropriate statistical test

**AI Analyst Intelligence**:
- Execute-first prevents fabrication
- Statistical rigor provides credible analysis
- Business context enables conversational insights
- Natural language generates authentic observations

### Design Philosophy Validated

**Core Innovation**: Semantic models that embed:
- Business logic (what metrics mean)
- Statistical rules (how to test claims)
- Contextual knowledge (how to interpret results)

**Result**: AI analyst that shows its work, quantifies uncertainty, and explores data like a real data scientist.

---

**Last Updated**: 2025-11-05
**Status**: Phase 2 Complete âœ… | Ready for Phase 3 ðŸš€
**Next**: MCP server implementation with FastMCP + Boring Semantic Layer